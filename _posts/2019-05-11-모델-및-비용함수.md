---
layout: article

title: 비용 함수

tags: machine-learning coursera week1

sidebar:

​	nav: layout

mathjax: true
---



# Cost Function

**비용 함수** 를 사용하여 hypothesis 함수의 정확성을 측정 할 수 있습니다 . 이것은 x와 실제 출력 y의 입력 값을 가진 가설의 모든 결과의 평균 차이 (평균보다 더 좋은 버전)를 취합니다. 

$$J(\theta_0, \theta_1) = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left ( \hat{y}_{i}- y_{i} \right)^2 = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left (h_\theta (x_{i}) - y_{i} \right)^2$$

이를 분석해보면, $$x¯$$가 $$h_\theta (x_{i}) - y_{i}hθ(xi)−yi$$ 의 제곱의 평균값일때 $$\frac{1}{2} x¯$$ 이거나, 실제값과 예측값의 차이 입니다.

이 함수는 "제곱 오류 함수"(Squared error function)또는 "평균 제곱 오류"(Mean squared error)라고도 합니다. 평균을 절반$$ \left(\frac{1}{2}\right)$$으로 나누었는데 이것은 gradient descent를 편리하게 계산하기 위함으로, 제곱 함수의 미분 항이$$\frac{1}{2}$$ 를 무효화 할 것입니다. 

다음 이미지는 비용 함수가 수행하는 작업을 요약 한 것입니다. 

![img](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R2YF5Lj3EeajLxLfjQiSjg_110c901f58043f995a35b31431935290_Screen-Shot-2016-12-02-at-5.23.31-PM.png?expiry=1557705600000&hmac=_dEVTgO9i7IgmXvcUtC-TPs_LKrsW8PogHSlRMTvlEU)

 

시각적인 관점에서 생각해 보면 학습 데이터 세트가 x-y 평면에 산재 해 있습니다. 우리는 직선을 만들기 위해( $h_\theta(x)$ 에 의해 정의된) 노력하고 있습니다 우리의 목표는 최상의 라인을 확보하는 것입니다. 가능한 최상의 선은 선에서부터 흩어진 점들과의 수직 거리의 제곱의 평균이 가장 작을 것입니다. 가장 좋은 경우에는 라인이 학습 데이터 세트의 모든 포인트를 통과할 것입니다. 그러한 경우에  $J(\theta_0, \theta_1)$ 는 0이됩니다. 